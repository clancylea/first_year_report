The principal objective of this research is to develop a system that performs 3D environment reconstruction using spiking neural networks. In order to achieve this goal, some milestones will have to be reached. 

The first step is to perform \emph{image classification}, something that experiments have shown can be done in about $150ms$ in the brain~\cite{Thorpe1996-speed-of-processing}. This is important for a real-time system as it enables certain known objects to act as markers in a three-dimensional environment. In order to achieve such classification speed, a \emph{temporal encoding} of information has been suggested in the literature~\cite{VanRullen2005-spike-times}. Although some work has been done to use this type of encoding, a learning procedure for artificial neural networks that use temporal coding is still an open research question and quite unexplored territory. Hierarchical networks have proven to be a robust way to recognize images~\cite{Behnke2003-hierachical-interpretation,Bengio2009-deep-architectures}, thus developing such a network seems like the most reasonable path. 
%The current state-of-the-art on image recognition and classification is deep belief networks \textbf{CITE!!!}, so comparing and analysing the different approaches is an obligatory task in this field of research. 

%\subsection{3D object recognition}
%\label{subsec:intro:plan:3D-recognition}
Given that different views of objects in the real world are correlated in space and time, spiking neural networks should make an excellent match for 3D object recognition. It would allow the environment reconstruction system to keep \emph{track of objects} regardless of their position, facilitating the localization part of the system. This is the second milestone for our research plan. % % % % % % % % % % % % % % % % % INVARIANT REPS

%\subsection{Depth perception}
%\label{subsec:intro:plan:depth-perception}
The third milestone is a way to establish the distance of objects to the observer or \emph{depth perception}. This could be done using binocular vision (either using two cameras or inferring the 3D transformation of the camera from optic flow), depth-from-defocus, or including other sensors~\cite{depth-from-binocular-defocus-vs,event-slam,tomasi1992shape}. 

%\subsection{Orientation and localization}
%\label{subsec:intro:plan:localization}
The localization and mapping problems have been proven to be easier to solve if taken simultaneously~\cite{Durrant-Whyte2006-slam,Fuentes-Pacheco2012-slam}. The common approach is to construct a probabilistic model from sensed data and special inference rules. Inspiration from rat hippocampus studies on location awareness have lead to neural network approaches~\cite{rat-slam}. From a neural networks point of view, the probabilistic models are stored in the network itself. Solving the \emph{simultaneous localization and mapping} (SLAM) problem using spiking neural networks is the fourth milestone.

%\subsection{Reconstruction}
%\label{subsec:intro:plan:reconstruction}
The final step of this research is to \emph{reconstruct an environment} from the knowledge stored in the neural network. This needs a top down approach, which is most commonly done by analysing network weights (sometimes using additional neural networks) and, from that, infer what the original input values were~\cite{Anh-Dung-deconv-nets}. 


%In summary, the project consists of employing spiking neural networks and spike-time encoding to perform:
%\begin{itemize}
%  \item Object recognition,
%  \item Object tracking,
%  \item Depth perception,
%  \item Orientation and localization, and
%  \item Environment reconstruction.
%\end{itemize}