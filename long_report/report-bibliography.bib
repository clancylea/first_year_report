@online{webvision-images,
    author = {Webvision},
    title = {Anatomical retina imagery source, modified to fit paper.},
    date = {2015},
    url = {http://webvision.med.utah.edu/},
}

@online{wikipedia-images,
  author = {Wikipedia},
  title = {Diagrams of brain and neural anatomy, modified to fit paper.},
  date = {2015},
  url = {http://wikipedia.org/},
}

@article{furber2014spinnaker,
  title={The spinnaker project},
  author={Furber, Steve B and Galluppi, Francesco and Temple, Sally and Plana, Luis and others},
  journal={Proceedings of the IEEE},
  volume={102},
  number={5},
  pages={652--665},
  year={2014},
  publisher={IEEE}
}

@inproceedings{spinnaker-review,
    author = {Rast, Alexander D. and Jin, Xin and Galluppi, Francesco and Plana, Luis A. and Patterson, Cameron and Furber, Steve},
    title = {Scalable Event-driven Native Parallel Processing: The SpiNNaker Neuromimetic System},
    booktitle = {Proceedings of the 7th ACM International Conference on Computing Frontiers},
    series = {CF '10},
    year = {2010},
    isbn = {978-1-4503-0044-5},
    location = {Bertinoro, Italy},
    pages = {21--30},
    numpages = {10},
    //url = {http://doi.acm.org/10.1145/1787275.1787279},
    doi = {10.1145/1787275.1787279},
    acmid = {1787279},
    publisher = {ACM},
    address = {New York, NY, USA},
    keywords = {asynchronous, event-driven, universal neural processor},
}

@article{furber2013overview,
  title={Overview of the spinnaker system architecture},
  author={Furber, Steve B and Lester, David R and Plana, Luis A and Garside, Jim D and Painkras, Eustace and Temple, Steve and Brown, Andrew D},
  journal={Computers, IEEE Transactions on},
  volume={62},
  number={12},
  pages={2454--2467},
  year={2013},
  publisher={IEEE}
}


///////////////////   retina hardware  ////////////////////
@INPROCEEDINGS{4145833,
  author={Nagy, Z. and Voroshazi, Z. and Szolgay, Peter},
  booktitle={Cellular Neural Networks and Their Applications, 2006. CNNA '06. 10th International Workshop on},
  title={A Real-time Mammalian Retina Model Implementation on FPGA},
  year={2006},
  month={Aug},
  pages={1-1},
  keywords={Application software;Cameras;Cellular neural networks;Displays;Field programmable gate arrays;Image processing;Laboratories;Partial differential equations;Retina;Very large scale integration;CNN-UM;FPGA;Retina Model},
  doi={10.1109/CNNA.2006.341593},}

@INPROCEEDINGS{1465812,
  author={Balya, D. and Roska, B.},
  booktitle={Circuits and Systems, 2005. ISCAS 2005. IEEE International Symposium on},
  title={Retina model with real time implementation},
  year={2005},
  month={May},
  pages={5222-5225 Vol. 5},
  keywords={cellular neural nets;eye;video signal processing;Bi-I stand-alone system;cellular nonlinear network;channel on-line control;channels spatial-temporal error;neuromorphic models;on-line parameter changing;retina model algorithmic skeleton;retina model real time implementation;video real time retina channels;Cellular neural networks;Computational modeling;Information technology;Neuromorphics;Neurons;Packaging;Power system modeling;Real time systems;Retina;Skeleton},
  doi={10.1109/ISCAS.2005.1465812},}


@MISC{Vogelstein07amultichip,
  author = {R. Jacob Vogelstein and Udayan Mallik and Eugenio Culurciello and Gert Cauwenberghs and Ralph Etienne-Cummings},
  title = {A Multichip Neuromorphic System for Spike-based Visual Information Processing},
  year = {2007}
}


////////////// intro //////////////
@article{Thrun2008_SLAM,
  author = {Thrun, Sebastian and Leonard, John},
  doi = {10.1007/978-3-540-30301-5\_38},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/slam/Simultaneous Localization and Mapping.pdf:pdf},
  isbn = {978-3-540-23957-4},
  journal = {Springer handbook of robotics},
  pages = {871--889},
  title = {{Simultaneous localization and mapping}},
  url = {http://link.springer.com/10.1007/978-3-540-30301-5\_38},
  year = {2008}
}

@article{VanRullen2005-spike-times,
  abstract = {Many behavioral responses are completed too quickly for the underlying sensory processes to rely on estimation of neural firing rates over extended time windows. Theoretically, first-spike times could underlie such rapid responses, but direct evidence has been lacking. Such evidence has now been uncovered in the human somatosensory system. We discuss these findings and their potential generalization to other sensory modalities, and we consider some future challenges for the neuroscientific community.},
  author = {VanRullen, Rufin and Guyonneau, Rudy and Thorpe, Simon J.},
  doi = {10.1016/j.tins.2004.10.010},
  file = {:opt/references/rank-order/Spike times make sense -- vanrullen-thorpe-2004.pdf:pdf},
  isbn = {0166-2236},
  issn = {01662236},
  journal = {Trends in Neurosciences},
  number = {1},
  pages = {1--4},
  pmid = {15626490},
  title = {{Spike times make sense}},
  volume = {28},
  year = {2005}
}

@misc{Thorpe1996-speed-of-processing,
  abstract = {How long does it take for the human visual system to process a complex natural image? Subjectively, recognition of familiar objects and scenes appears to be virtually instantaneous, but measuring this processing time experimentally has proved difficult. Behavioural measures such as reaction times can be used, but these include not only visual processing but also the time required for response execution. However, event-related potentials (ERPs) can sometimes reveal signs of neural processing well before the motor output. Here we use a go/no-go categorization task in which subjects have to decide whether a previously unseen photograph, flashed on for just 20 ms, contains an animal. ERP analysis revealed a frontal negativity specific to no-go trials that develops roughly 150 ms after stimulus onset. We conclude that the visual processing needed to perform this highly demanding task can be achieved in under 150 ms.},
  author = {Thorpe, S and Fize, D and Marlot, C},
  booktitle = {Nature},
  doi = {10.1038/381520a0},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/vision/speed of processing in the human visual system --- SpeedOfProcessing.pdf:pdf},
  isbn = {0028-0836 (Print)$\backslash$r0028-0836 (Linking)},
  issn = {0028-0836},
  number = {6582},
  pages = {520--522},
  pmid = {8632824},
  title = {{Speed of processing in the human visual system.}},
  volume = {381},
  year = {1996}
}

@book{Behnke2003-hierachical-interpretation,
  abstract = {This document contains an extended abstract of my dissertation thesis. The thesis was supervised by Prof. Dr. Ra´ul Rojas (FU Berlin). My profound gratitude goes to him for guidance, contribution of ideas, and encouragement. The thesis was submitted in October 2002 and defended in November of the same year. Prof. Dr. Volker Sperschneider (Osnabr¨uck) was its second referee.},
  author = {Behnke, Sven},
  booktitle = {Lecture Notes in Computer Science},
  doi = {10.1287/mksc.1060.0207},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/books/Hierarchical Neural Networks for Image Interpretation ---- LNCS2766.pdf:pdf},
  isbn = {3540407227},
  issn = {03029743},
  pages = {244},
  pmid = {17704750},
  title = {{Hierarchical Neural Networks for Image Interpretation}},
  volume = {2766},
  year = {2003}
}

@article{Bengio2009-deep-architectures,
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  archivePrefix = {arXiv},
  arxivId = {submit/0500581},
  author = {Bengio, Yoshua},
  doi = {10.1561/2200000006},
  eprint = {0500581},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/deep-learning/deep learning architectures --- ftml.pdf:pdf},
  isbn = {2200000006},
  issn = {1935-8237},
  journal = {Foundations and Trends® in Machine Learning},
  number = {1},
  pages = {1--127},
  pmid = {17348934},
  primaryClass = {submit},
  title = {{Learning Deep Architectures for AI}},
  volume = {2},
  year = {2009}
}

@article{Anh-Dung-deconv-nets,
  abstract = {My notes about DN},
  author = {Anh-Dung, Doan},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/slam/Deconvolutional Networks.pdf:pdf},
  isbn = {9781424469857},
  title = {{Deconvolutional Networks}}
}


@article{Milford2004-ratslam,
  abstract = {The paper presents a new approach to the problem of Simultaneous Localization and Mapping - SLAM - inspired by computational models of the hippocampus of rodents. The rodents hippocampus has been extensively studied with respect to navigation tasks, and displays many of the properties of a desirable SLAM solution. RatSLAM is an implementation of a hippocampal model that can perform SLAM in real time on a real robot. It uses competitive attractor network to integrate odometric information with landmark sensing to form a consistent representation of the environment. Experimental results show that RatSLAM can operate with ambiguous landmark information and recover from both minor and major path integration errors.},
  author = {Milford, Mj and Wyeth, Gf and Prasser, D},
  doi = {10.1109/ROBOT.2004.1307183},
  file = {:opt/references/slam/RatSLAM --- c37593.pdf:pdf},
  isbn = {0780382323},
  issn = {1050-4729},
  journal = {Robotics and Automation, \ldots},
  keywords = {hippocampus,mobile robot,slam},
  number = {May 2004},
  pages = {403--408},
  title = {{RatSLAM: a hippocampal model for simultaneous localization and mapping}},
  url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1307183},
  year = {2004}
}


@article{Durrant-Whyte2006-slam,
  abstract = {This paper describes the simultaneous localization and mapping (SLAM) problem and the essential methods for solving the SLAM problem and summarizes key implementations and demonstrations of the method. While there are still many practical issues to overcome, especially in more complex outdoor environments, the general SLAM method is now a well understood and established part of robotics. Another part of the tutorial summarized more recent works in addressing some of the remaining issues in SLAM, including computation, feature representation, and data association},
  archivePrefix = {arXiv},
  arxivId = {there is not},
  author = {Durrant-Whyte, H and Bailey, T},
  doi = {10.1109/MRA.2006.1638022},
  eprint = {there is not},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/slam/Simultaneous Localization and Mapping -\_- part 1.pdf:pdf},
  isbn = {1070-9932},
  issn = {1070-9932},
  journal = {IEEE Robotics \& Automation Magazine},
  number = {2},
  pages = {99--116},
  pmid = {1638022},
  title = {{Simultaneous localization and mapping (SLAM)}},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1638022},
  volume = {13},
  year = {2006}
}
@article{Fuentes-Pacheco2012-slam,
  abstract = {Visual SLAM (simultaneous localization and mapping) refers to the problem of using images, as the only source of external information, in order to establish the position of a robot, a vehicle, or a moving camera in an environment, and at the same time, construct a representation of the explored zone. SLAM is an essential task for the autonomy of a robot. Nowadays, the problem of SLAM is considered solved when range sensors such as lasers or sonar are used to built 2D maps of small static environments. However SLAM for dynamic, complex and large scale environments, using vision as the sole external sensor, is an active area of research. The computer vision techniques employed in visual SLAM, such as detection, description and matching of salient features, image recognition and retrieval, among others, are still susceptible of improvement. The objective of this article is to provide new researchers in the field of visual SLAM a brief and comprehensible review of the state-of-the-art. © 2012 Springer Science+Business Media Dordrecht.},
  author = {Fuentes-Pacheco, Jorge and Ruiz-Ascencio, Jos\'{e} and Rend\'{o}n-Mancha, Juan Manuel},
  doi = {10.1007/s10462-012-9365-8},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/slam/Visual simultaneous localization and mapping -\_- a survey.pdf:pdf},
  isbn = {02692821 (ISSN)},
  issn = {02692821},
  journal = {Artificial Intelligence Review},
  keywords = {Data association,Image matching,Salient feature selection,Topological and metric maps,Visual SLAM},
  pages = {1--27},
  title = {{Visual simultaneous localization and mapping: a survey}},
  year = {2012}
}


@article{depth-from-binocular-defocus-vs,
  year={2000},
  issn={0920-5691},
  journal={International Journal of Computer Vision},
  volume={39},
  number={2},
  doi={10.1023/A:1008175127327},
  title={Depth from Defocus vs. Stereo: How Different Really Are They?},
  url={http://dx.doi.org/10.1023/A%3A1008175127327},
  publisher={Kluwer Academic Publishers},
  keywords={Defocus; depth from focus; depth of field; depth sensing; range imaging; shape from X; stereo; triangulation},
  author={Schechner, YoavY. and Kiryati, Nahum},
  pages={141-162},
  language={English}
}

@article{event-slam,
  author = {Weikersdorfer, David and Adrian, David B and Cremers, Daniel},
  doi = {10.1109/ICRA.2014.6906882},
  file = {:opt/references/sensor-fusion/Event-based 3D SLAM with a depth-augmented dynamic vision sensor.pdf:pdf},
  isbn = {9781479936847},
  issn = {10504729},
  journal = {2014 IEEE International Conference on Robotics and Automation},
  pages = {359--364},
  title = {{Event-based 3D SLAM with a depth-augmented dynamic vision sensor}},
  year = {2014}
}

@article{tomasi1992shape,
  title={Shape and motion from image streams under orthography: a factorization method},
  author={Tomasi, Carlo and Kanade, Takeo},
  journal={International Journal of Computer Vision},
  volume={9},
  number={2},
  pages={137--154},
  year={1992},
  publisher={Springer}
}

@article{rat-slam,
  abstract = {The paper presents a new approach to the problem of Simultaneous Localization and Mapping - SLAM - inspired by computational models of the hippocampus of rodents. The rodents hippocampus has been extensively studied with respect to navigation tasks, and displays many of the properties of a desirable SLAM solution. RatSLAM is an implementation of a hippocampal model that can perform SLAM in real time on a real robot. It uses competitive attractor network to integrate odometric information with landmark sensing to form a consistent representation of the environment. Experimental results show that RatSLAM can operate with ambiguous landmark information and recover from both minor and major path integration errors.},
  author = {Milford, Mj and Wyeth, Gf and Prasser, D},
  doi = {10.1109/ROBOT.2004.1307183},
  file = {:opt/references/slam/RatSLAM --- c37593.pdf:pdf},
  isbn = {0780382323},
  issn = {1050-4729},
  journal = {Robotics and Automation, \ldots},
  keywords = {hippocampus,mobile robot,slam},
  number = {May 2004},
  pages = {403--408},
  title = {{RatSLAM: a hippocampal model for simultaneous localization and mapping}},
  url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1307183},
  year = {2004}
}
