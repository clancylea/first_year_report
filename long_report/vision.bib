@article{deep-nets-hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{lowe2004distinctive,
  title={Distinctive image features from scale-invariant keypoints},
  author={Lowe, David G},
  journal={International journal of computer vision},
  volume={60},
  number={2},
  pages={91--110},
  year={2004},
  publisher={Springer}
}

@inproceedings{lowe1999object,
  title={Object recognition from local scale-invariant features},
  author={Lowe, David G},
  booktitle={Computer vision, 1999. The proceedings of the seventh IEEE international conference on},
  volume={2},
  pages={1150--1157},
  year={1999},
  organization={Ieee}
}

@inproceedings{alahi2012freak,
  title={Freak: Fast retina keypoint},
  author={Alahi, Alexandre and Ortiz, Raphael and Vandergheynst, Pierre},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages={510--517},
  year={2012},
  organization={Ieee}
}

@online{webvision-gross-eye,
  author = {Kolb, Helga},
  editor = {WebVision, Moran Eye Center},
  title = {Gross anatomy of the eye},
  date = {2015},
  url = {http://webvision.med.utah.edu/book/part-i-foundations/gross-anatomy-of-the-ey/},
}

@online{webvision-simple-retina,
  author = {Kolb, Helga},
  editor = {WebVision, Moran Eye Center},
  title = {Simple anatomy of the retina},
  date = {2015},
  url = {http://webvision.med.utah.edu/book/part-i-foundations/simple-anatomy-of-the-retina/},
}

@article{learning-deep-Bengio2009,
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  archivePrefix = {arXiv},
  arxivId = {submit/0500581},
  author = {Bengio, Yoshua},
  doi = {10.1561/2200000006},
  eprint = {0500581},
  file = {:run/user/26077/gvfs/smb-share$\backslash$:domain=ds.man.ac.uk,server=nask.man.ac.uk,share=home\$,user=mbgppgp7/references/deep-learning/deep learning architectures --- ftml.pdf:pdf},
  isbn = {2200000006},
  issn = {1935-8237},
  journal = {Foundations and TrendsÂ® in Machine Learning},
  number = {1},
  pages = {1--127},
  pmid = {17348934},
  primaryClass = {submit},
  title = {{Learning Deep Architectures for AI}},
  volume = {2},
  year = {2009}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press}
}


@INPROCEEDINGS{autoencoders-lecun2007,
  author={Ranzato, M. and Fu Jie Huang and Boureau, Y.-L. and LeCun, Y.},
  booktitle={Computer Vision and Pattern Recognition, 2007. CVPR '07. IEEE Conference on},
  title={Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition},
  year={2007},
  month={June},
  pages={1-8},
  keywords={feature extraction;object recognition;unsupervised learning;feature extractor;feature-pooling layer;invariant feature hierarchy;multiple convolution filters;object recognition;unsupervised learning;Computer architecture;Computer vision;Convolution;Detectors;Feature extraction;Gabor filters;Object detection;Object recognition;Supervised learning;Unsupervised learning},
  doi={10.1109/CVPR.2007.383157},
  ISSN={1063-6919},
}

@article{evangelos-deep-belief,
  abstract = {Increasingly large deep learning architectures, such as Deep Belief Networks (DBNs) are the focus of current machine learning research and achieve state-of-the-art results in different domains. However, both training and execution of large-scale Deep Networks require vast computing resources, leading to high power requirements and communication overheads. The on-going work on design and construction of spike-based hardware platforms offers an alternative for running deep neural networks with significantly lower power consumption, but has to overcome hardware limitations in terms of noise and limited weight precision, as well as noise inherent in the sensor signal. This article investigates how such hardware constraints impact the performance of spiking neural network implementations of DBNs. In particular, the influence of limited bit precision during execution and training, and the impact of silicon mismatch in the synaptic weight parameters of custom hybrid VLSI implementations is studied. Furthermore, the network performance of spiking DBNs is characterized with regard to noise in the spiking input signal. Our results demonstrate that spiking DBNs can tolerate very low levels of hardware bit precision down to almost two bits, and show that their performance can be improved by at least 30\% through an adapted training mechanism that takes the bit precision of the target platform into account. Spiking DBNs thus present an important use-case for large-scale hybrid analog-digital or digital neuromorphic platforms such as SpiNNaker, which can execute large but precision-constrained deep networks in real time.},
  author = {Stromatias, Evangelos and Neil, Daniel and Pfeiffer, Michael and Galluppi, Francesco and Furber, Steve B and Liu, Shih-Chii},
  doi = {10.3389/fnins.2015.00222},
  issn = {1662-453X},
  journal = {Frontiers in Neuroscience},
  number = {222},
  title = {{Robustness of spiking Deep Belief Networks to noise and reduced bit precision of neuro-inspired hardware platforms}},
  url = {http://www.frontiersin.org/neuromorphic\_engineering/10.3389/fnins.2015.00222/abstract},
  volume = {9},
  year = {2015}
}

@article{diehlfast-deep-net,
  title={Fast-Classifying, High-Accuracy Spiking Deep Networks Through Weight and Threshold Balancing},
  author={Diehl, Peter U and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael}
}


@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}

@book{hebb2005organization,
  title={The organization of behavior: A neuropsychological theory},
  author={Hebb, Donald Olding},
  year={2005},
  publisher={Psychology Press}
}

@article{hubel1962receptive,
  title={Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  author={Hubel, David H and Wiesel, Torsten N},
  journal={The Journal of physiology},
  volume={160},
  number={1},
  pages={106},
  year={1962},
  publisher={Blackwell Publishing}
}
