@online{int-perf-images,
  author = {Henk Poley},
  title = {Integer performance},
  date = {2015},
  url = {http://preshing.com/20120208/a-look-back-at-single-threaded-cpu-performance/},
  note = {https://twitter.com/HenkPoley},
}

@online{mips-perf-image,
  title={Advanced architecture online course},
  date={2015},
  url={http://cs.mwsu.edu/~terry/?route=courses/5133/}
}

@article{nickolls2010gpu,
  title={The GPU computing era},
  author={Nickolls, John and Dally, William J},
  journal={IEEE micro},
  number={2},
  pages={56--69},
  year={2010},
  publisher={IEEE}
}

@inproceedings{chen2009gpu,
  title={GPU technology trends and future requirements},
  author={Chen, John Y},
  booktitle={Electron Devices Meeting (IEDM), 2009 IEEE International},
  pages={1--6},
  year={2009},
  organization={IEEE}
}

@article{deep-blue-Campbell200257,
  abstract = {Deep Blue is the chess machine that defeated then-reigning World Chess Champion Garry Kasparov in a six-game match in 1997. There were a number of factors that contributed to this success, including: • a single-chip chess search engine, • a massively parallel system with multiple levels of parallelism, • a strong emphasis on search extensions, • a complex evaluation function, and • effective use of a Grandmaster game database. This paper describes the Deep Blue system, and gives some of the rationale that went into the design decisions behind Deep Blue. },
  author = {Campbell, Murray and Jr., A.Joseph Hoane and Hsu, Feng-hsiung},
  doi = {http://dx.doi.org/10.1016/S0004-3702(01)00129-1},
  issn = {0004-3702},
  journal = {Artificial Intelligence},
  keywords = {Computer chess,Evaluation function,Game tree search,Parallel search,Search extensions,Selective search},
  number = {1–2},
  pages = {57--83},
  title = {{Deep Blue}},
  url = {http://www.sciencedirect.com/science/article/pii/S0004370201001291},
  volume = {134},
  year = {2002}
}


@article{indiveri2011frontiers,
  title={Frontiers in neuromorphic engineering},
  author={Indiveri, Giacomo and Horiuchi, Timothy K},
  journal={Frontiers in neuroscience},
  volume={5},
  year={2011},
  publisher={Frontiers Media SA}
}

@book{mead2012analog,
  title={Analog VLSI implementation of neural systems},
  author={Mead, Carver and Ismail, Mohammed},
  volume={80},
  year={2012},
  publisher={Springer Science \& Business Media}
}


@article{liu2010neuromorphic,
  title={Neuromorphic sensory systems},
  author={Liu, Shih-Chii and Delbruck, Tobi},
  journal={Current opinion in neurobiology},
  volume={20},
  number={3},
  pages={288--295},
  year={2010},
  publisher={Elsevier}
}


@online{nvidia,
  title = {Nvidia GeForce GTX Titan Z specifications},
  date = {2015},
  url = {http://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-titan-z/specifications},
}

@online{amd,
  title = {Asus/AMD R9390 specifications},
  date = {2015},
  url = {https://www.asus.com/Graphics-Cards/R9390DC28GD5/specifications/},
}

@inproceedings{brainscales-schemmel2010wafer,
  title={A wafer-scale neuromorphic hardware system for large-scale neural modeling},
  author={Schemmel, Johannes and Bruderle, D and Grubl, A and Hock, Matthias and Meier, Karlheinz and Millner, Sebastian},
  booktitle={Circuits and Systems (ISCAS), Proceedings of 2010 IEEE International Symposium on},
  pages={1947--1950},
  year={2010},
  organization={IEEE}
}

@online{brainscales-homepage,
  title = {BrainScales project},
  date = {2015},
  url = {https://brainscales.kip.uni-heidelberg.de/}
}

@ARTICLE{neurogrid-6805187,
  author={Benjamin, B.V. and Peiran Gao and McQuinn, E. and Choudhary, S. and Chandrasekaran, A.R. and Bussat, J.-M. and Alvarez-Icaza, R. and Arthur, J.V. and Merolla, P.A. and Boahen, K.},
  journal={Proceedings of the IEEE},
  title={Neurogrid: A Mixed-Analog-Digital Multichip System for Large-Scale Neural Simulations},
  year={2014},
  month={May},
  volume={102},
  number={5},
  pages={699-716},
  keywords={analogue circuits;biology computing;digital circuits;network theory (graphs);neurophysiology;trees (mathematics);Neurogrid system;axonal arbor element;biological neural systems;dendritic tree element;electronic circuits;large-scale neural simulations;mixed-analog-digital multichip system;neural elements;neuromorphic systems;soma element;synapse element;synaptic connections;tree network;Computer architecture;Electronic circuits;Integrated circuit modeling;Nerve fibers;Neural networks;Neuroscience;Random access memory;Synchronous digital hierarchy;Analog circuits;application specific integrated circuits;asynchronous circuits;brain modeling;computational neuroscience;interconnection networks;mixed analog-digital integrated circuits;neural network hardware;neuromorphic electronic systems},
  doi={10.1109/JPROC.2014.2313565},
  ISSN={0018-9219},}

@article{truenorth-merolla2014million,
  title={A million spiking-neuron integrated circuit with a scalable communication network and interface},
  author={Merolla, Paul A and Arthur, John V and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and others},
  journal={Science},
  volume={345},
  number={6197},
  pages={668--673},
  year={2014},
  publisher={American Association for the Advancement of Science}
}

@online{truenorth-web,
  title={IBM Research: Brain-inspired Chip},
  date={2015},
  url={http://www.research.ibm.com/articles/brain-chip.shtml},
}
@article{misra2010artificial,
  title={Artificial neural networks in hardware: A survey of two decades of progress},
  author={Misra, Janardan and Saha, Indranil},
  journal={Neurocomputing},
  volume={74},
  number={1},
  pages={239--255},
  year={2010},
  publisher={Elsevier}
}



@ARTICLE{neuro-platforms-summary-7159144,
  author={Indiveri, G. and Liu, S.},
  journal={Proceedings of the IEEE},
  title={Memory and Information Processing in Neuromorphic Systems},
  year={2015},
  month={Aug},
  volume={103},
  number={8},
  pages={1379-1397},
  keywords={Biological neural networks;Brain modeling;Computer architecture;Field programmable gate arrays;Information processing;Memory management;Neuromorphics;Neurons;Program processors;Convolutional networks;VLSI;deep neural networks (DNNs);event-based computation;learning;massively parallel;memristor;neuromorphic computing;plasticity;spike-timing-dependent plasticity (STDP);spiking neural network (SNN);von Neumann bottleneck},
  doi={10.1109/JPROC.2015.2444094},
  ISSN={0018-9219},}

@ARTICLE{to-build-a-brain-6247562,
  author={Furber, S.},
  journal={Spectrum, IEEE},
  title={To build a brain},
  year={2012},
  month={August},
  volume={49},
  number={8},
  pages={44-49},
  keywords={brain;digital circuits;mainframes;parallel machines;digital circuits;human brain;neurons;nuclear power plant;supercomputer;synapses;wetware;Analog circuits;Computational modeling;Neurons;Nuclear power generation;Program processors},
  doi={10.1109/MSPEC.2012.6247562},
  ISSN={0018-9235},}

@article{spinn-net-patterson2012scalable,
  title={Scalable communications for a million-core neural processing architecture},
  author={Patterson, Cameron and Garside, Jim and Painkras, Eustace and Temple, Steve and Plana, Luis A and Navaridas, Javier and Sharp, Thomas and Furber, Steve},
  journal={Journal of Parallel and Distributed Computing},
  volume={72},
  number={11},
  pages={1507--1520},
  year={2012},
  publisher={Elsevier}
}

@article{davison2008pynn,
  title={PyNN: a common interface for neuronal network simulators},
  author={Davison, Andrew P and Br{\"u}derle, Daniel and Eppler, Jochen and Kremkow, Jens and Muller, Eilif and Pecevski, Dejan and Perrinet, Laurent and Yger, Pierre},
  journal={Frontiers in neuroinformatics},
  volume={2},
  year={2008},
  publisher={Frontiers Research Foundation}
}

@online{spinn-software-docs,
  title={SpiNNaker software documents},
  date={2015},
  url={https://spinnaker.cs.manchester.ac.uk/tiki-index.php?page=SpiNNaker%20software%20documents}
}

@online{spynnaker-github,
  title={SpiNNaker Manchester at github.com},
  date={2015},
  url={http://spinnakermanchester.github.io}
}

@inproceedings{esmaeilzadeh2011dark,
  title={Dark silicon and the end of multicore scaling},
  author={Esmaeilzadeh, Hadi and Blem, Emily and Amant, Renee St and Sankaralingam, Karthikeyan and Burger, Doug},
  booktitle={Computer Architecture (ISCA), 2011 38th Annual International Symposium on},
  pages={365--376},
  year={2011},
  organization={IEEE}
}

@online{riken-nest,
 title={Largest neuronal network simulation achieved using K computer},
 date={2015},
 url={http://www.riken.jp/en/pr/press/2013/20130802_1/},
}

@ARTICLE{rolls-processor,
  AUTHOR={Qiao, Ning  and  Mostafa, Hesham  and  Corradi, Federico  and  Osswald, Marc  and  Stefanini, Fabio  and  Sumislawska, Dora  and  Indiveri, Giacomo},   
  TITLE={A Re-configurable On-line Learning Spiking Neuromorphic Processor comprising 256 neurons and 128K synapses},      
  JOURNAL={Frontiers in Neuroscience},      
  VOLUME={9},      
  YEAR={2015},      
  NUMBER={141},     
  URL={http://www.frontiersin.org/neuromorphic_engineering/10.3389/fnins.2015.00141/abstract}, 
  DOI={10.3389/fnins.2015.00141},      
  ISSN={1662-453X} ,      
  ABSTRACT={Implementing compact, low-power artificial neural processing systems with real-time on-line learning abilities is still an open challenge. In this paper we present a full-custom mixed-signal VLSI device with neuromorphic learning circuits that emulate the biophysics of real spiking neurons and dynamic synapses for exploring the properties of computational neuroscience models and for building brain-inspired computing systems. The proposed architecture allows the on-chip configuration of a wide range of network connectivities, including recurrent and deep networks, with short-term and long-term plasticity. The device comprises 128 K analog synapse and 256 neuron circuits with biologically plausible dynamics and bi-stable spike-based plasticity mechanisms that endow it with on-line learning abilities. In addition to the analog circuits, the device comprises also asynchronous digital logic circuits for setting different synapse and neuron properties as well as different network configurations. This prototype device, fabricated using a 180 nm 1P6M CMOS process, occupies an area of 51.4 mm2, and consumes approximately 4 mW for typical experiments, for example involving attractor networks. Here we describe the details of the overall architecture and of the individual circuits and present experimental results that showcase its potential. By supporting a wide range of cortical-like computational modules comprising plasticity mechanisms, this device will enable the realization of intelligent autonomous systems with on-line learning capabilities.}
}