\documentclass[12t,a4paper]{memoir}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
%\usepackage[left=2cm, right=2cm]{geometry}

\begin{document}


Hello, my name is Garibaldi Pineda Garcia and during this presentation I'll be 
talking about my PhD project plan and the progress done so far. My research is about computer vision on the SpiNNaker platform, in particular the simultaneous localization and mapping problem. \\

{\Large NEXT SLIDE}\\

When I was young, I used to spend ours trying to find Wally in every book I owned; now I believe that it's a way of training our brain to identify everyday scenarios (though I still enjoy the books).\\

Most of the pages in the books involved cities, and I think it's because we have shaped the world to fit our visual nature. For me vision is important, because most of us use our eyes in everyday activities like dressing, getting to work or watching films.\\

{\Large NEXT SLIDE}\\

The goal of my PhD is to solve the Simultaneous Localization and Mapping or SLAM problem using spiking neural networks. \\

SLAM algorithms have been used in navigation, augmented reality, and environment reconstruction applications, to name a few. For example, ff we have a robot and want to tell it where to go, the robot needs to figure out where it is how to get to its destination.\\
%While having a map to guide itself is useful, it also needs to know it is located within that map. 

A SLAM algorithm would allow the robot to both estimate its position and create a representation of the environment. Once it develops a map of the environment, it can fit the destination in and estimate a path. \\

{\Large NEXT SLIDE}\\

In general a SLAM algorithm has three stages. First, the robot would take measurements from the world and its own motion sensors. These are known as external and internal variables, respectively. Later, it would fit the new evidence into a model (the most popular ones are probabilistic, particle and graph models). Finally, it would update the current estimation of its position and the mapping. \\

{\Large NEXT SLIDE}\\

From our review of conventional SLAM algorithms, we can say that they are not mobile friendly. The main reason is that they require high performance hardware for real-time applications.\\

We also noted that advanced solutions require exotic sensors, such as the huge laser range scanner on top of Google's cars; or Kinect-like cameras that only work indoors.\\

In this regard, our approach would use regular cameras as the main external input of the system.\\

{\Large NEXT SLIDE}\\

There's evidence that a neural network approach to this problem, can be more energy efficient while keeping good results. The algorithm known as ratSLAM is based on observations of rodent's brains while traversing labyrinths.\\

Spiking neural networks are closer to their biological counterparts, and could enable the use of neuromorphic hardware, like SpiNNaker. This can lead to further reduction on energy consumption. Since our system will be based on spiking neural networks and we are imposing ourselves biological plausibility constraints, this research could also throw some light into the function of the brain.\\

{\Large NEXT SLIDE}\\

Our version of the SLAM algorithm would first have to encode visual input into a  neural representation, which is fed into a classifier network to identify cues about the robot position based on what landmarks are seen.\\

From this identification process, stimuli are sent to another network whose principal characteristic is that its neurons respond when specific places are visited or a robot orientation is sensed. These networks keep track of the robot's position, and also store a neural version of a map. Although this map is not cartesian, the robot could still use it to navigate.\\

{\Large NEXT SLIDE}\\

Now we will review the work developed during this year. Since we are using vision as the input of the system, we developed a couple of video encoders using GPUs and OpenCL. Our task this year was to convert a conventional camera's output into a spike representation. These representations are characterized for having at most a single spike per pixel, per frame.\\

{\Large NEXT SLIDE}\\

The first encoder is based on work done here, in the APT group. It is based on the physiology of a high-resolution area of the retina. One of its features is that its able to retain visual information after encoding, this means that we can reconstruct an image from the spike representation.\\

The spike representation is shown as the first four images, and the reconstruction is the rightmost picture. So far, we encode at 12 frames per second, for the first part of the algorithm, and the last part, though slowly, can be computed in-line. Although this performance might seem low, the encoder acts as the first burst of spikes generated by the eye immediately after a rapid movement, which only happens about 3 times per second.\\

{\Large NEXT SLIDE}\\

The second encoder is inspired by a neuromorphic device known as a dynamic vision sensor, which detects contrast changes and generates a spike for pixels whose intensity has changed above a threshold value.\\

In our encoder, the threshold value is adapted every frame to allow slow changing pixels to spike, and also to diminish the activity of pixels that change too fast. We can encode video in real-time and the output is similar to the constant information output of the eye between rapid movements.\\

{\Large NEXT SLIDE}\\

We've familiarized with the SpiNNaker platform and covered the initial literature review. \\

During this year, we developed two encoders, which are complementary and will be merged into a single system, that will act as input to our SLAM algorithm. We are currently studying a way to implement the encoders on SpiNNaker, as well as on custom hardware.\\

One of the encoders was used to create part of a database of spiking visual input, which is reported in an article that will be submitted for revision on the Frontiers of Neuroscience journal.\\


{\Large NEXT SLIDE}\\

We still have lots of work to do, the most relevant aspects are the design of a deep network for object recognition and tracking. This network has to learn the patterns it recognizes, which is commonly done off-line. We have set our mind in performing on-line learning with spiking neural networks, which is still an open research question. \\

After the learning and classification network stages of the research plan have been completed, we will merge the network with another that allows the robot to keep track of its position and estimate a map of the environment.\\

{\Large NEXT SLIDE}\\

This concludes my presentation, if you have any comments or questions, please feel free to express them now.\\






\end{document}