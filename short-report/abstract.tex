Converting frame-based video into spike-trains is a computationally intensive 
and time consuming task. There are very few examples of equipment that can 
produce spikes from video and they are either in development, use custom 
hardware or too expensive, thus every-day users are put behind a virtual wall, 
leaving them unable to experiment with visual input in their simulations or 
robotics applications. An efficient parallel implementation of a simple yet 
powerful retinal model would remove this wall by reducing the time it takes to 
compute a spike representation of a video frame. Furthermore if this is done on 
consumer graphics processing hardware, it will allow almost any user to 
generate their own spike-trains. 